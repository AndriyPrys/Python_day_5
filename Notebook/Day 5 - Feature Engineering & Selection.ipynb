{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d3ed21",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img style=\"float: left\"; src=\"img\\bip.jpeg\" width=\"60\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Day 5 - Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507b8d1",
   "metadata": {},
   "source": [
    "# Content:\n",
    "[1. Introduction](#1)  <br>\n",
    "[2. Label encoding](#2)  <br>\n",
    "[3. One hot encoding](#3)  <br>\n",
    "[4. Feature scaling](#4)  <br>\n",
    "[5. Feature selection methods](#5)  <br>\n",
    " - [Reduced Variance Feature Selection](#5.1)  <br>\n",
    " - [Univariate feature selection](#5.2)  <br>\n",
    " - [Principal Component Analysis](#5.3)  <br>\n",
    " - [Random Forest](#5.4)  <br>\n",
    " - [Lasso regularization](#5.5)  <br>\n",
    " - [Recursive Feature Elimination](#5.6)  <br>\n",
    " \n",
    "[6. Unbalanced classes](#6)  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bad81c",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# 1. Introduction\n",
    "\n",
    "### Feature engineering\n",
    "What is a feature and why we need the engineering of it? Basically, all machine learning algorithms use some input data to create outputs. This input data comprise features, which are usually in the form of structured columns. Algorithms require features with some specific characteristic to work properly. Here, the need for **feature engineering** arises. \n",
    "Feature engineering efforts mainly have two goals:\n",
    "- Preparing the proper input dataset, compatible with the machine learning algorithm requirements.\n",
    "- Improving the performance of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df8631",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "**Feature selection** is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.\n",
    "\n",
    "Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.\n",
    "\n",
    "Three benefits of performing feature selection before modeling your data are:\n",
    "\n",
    "- Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "- Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "- Reduces Training Time: Less data means that algorithms train faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe27db9",
   "metadata": {},
   "source": [
    "### The Dataset: _Credit Card Clients Dataset_\n",
    "\n",
    "Today we focus on feature engineering and selection techniques applied to the dataset we prepared last lessons.\n",
    "\n",
    "This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\n",
    "\n",
    "#### Content\n",
    "There are 28 variables:\n",
    "\n",
    "- ID: ID of each client\n",
    "\n",
    "- LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit)\n",
    "\n",
    "- SEX: Gender (male, female)\n",
    "\n",
    "- EDUCATION: (0=unknown, 1=graduate school, 2=university, 3=high school, 4=others)\n",
    "\n",
    "- MARRIAGE: Marital status (unknown, married, single, others)\n",
    "\n",
    "- AGE: Age in years\n",
    "\n",
    "- PAY_0: Repayment status in September, 2005 (-2=no consumption, -1=paid in full,0=paid the minimum due amount, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "\n",
    "- PAY_2: Repayment status in August, 2005 (scale same as above)\n",
    "\n",
    "- PAY_3: Repayment status in July, 2005 (scale same as above)\n",
    "\n",
    "- PAY_4: Repayment status in June, 2005 (scale same as above)\n",
    "\n",
    "- PAY_5: Repayment status in May, 2005 (scale same as above)\n",
    "\n",
    "- PAY_6: Repayment status in April, 2005 (scale same as above)\n",
    "\n",
    "- BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
    "\n",
    "- BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
    "\n",
    "- BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
    "\n",
    "- BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
    "\n",
    "- BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
    "\n",
    "- BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
    "\n",
    "- PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
    "\n",
    "- PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
    "\n",
    "- PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
    "\n",
    "- PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
    "\n",
    "- PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
    "\n",
    "- PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
    "\n",
    "- default: Default payment (1=yes, 0=no)\n",
    "\n",
    "- age_group: Age category (young, middle, senior)\n",
    " \n",
    "- credit_lev: Credit level (bronze, silver, gold)\n",
    "\n",
    "- pay_avg: Average number of payment default months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f7089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first of all we need to import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "from data.support import *\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe4d08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>limit_bal</th>\n",
       "      <th>sex</th>\n",
       "      <th>education</th>\n",
       "      <th>marriage</th>\n",
       "      <th>age</th>\n",
       "      <th>pay_0</th>\n",
       "      <th>pay_2</th>\n",
       "      <th>pay_3</th>\n",
       "      <th>pay_4</th>\n",
       "      <th>pay_5</th>\n",
       "      <th>pay_6</th>\n",
       "      <th>bill_amt1</th>\n",
       "      <th>bill_amt2</th>\n",
       "      <th>bill_amt3</th>\n",
       "      <th>bill_amt4</th>\n",
       "      <th>bill_amt5</th>\n",
       "      <th>bill_amt6</th>\n",
       "      <th>pay_amt1</th>\n",
       "      <th>pay_amt2</th>\n",
       "      <th>pay_amt3</th>\n",
       "      <th>pay_amt4</th>\n",
       "      <th>pay_amt5</th>\n",
       "      <th>pay_amt6</th>\n",
       "      <th>default</th>\n",
       "      <th>age_group</th>\n",
       "      <th>credit_lev</th>\n",
       "      <th>pay_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>married</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>3913</td>\n",
       "      <td>3102</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>young</td>\n",
       "      <td>bronze</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>single</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2682</td>\n",
       "      <td>1725</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>young</td>\n",
       "      <td>silver</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>single</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29239</td>\n",
       "      <td>14027</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>young</td>\n",
       "      <td>silver</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>married</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46990</td>\n",
       "      <td>48233</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>middle</td>\n",
       "      <td>bronze</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>married</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8617</td>\n",
       "      <td>5670</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "      <td>senior</td>\n",
       "      <td>bronze</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  limit_bal     sex  education marriage  age  pay_0  pay_2  pay_3  pay_4  pay_5  pay_6  bill_amt1  bill_amt2  bill_amt3  bill_amt4  bill_amt5  bill_amt6  pay_amt1  pay_amt2  pay_amt3  pay_amt4  pay_amt5  pay_amt6  default age_group credit_lev   pay_avg\n",
       "0   1    20000.0  female          2  married   24      2      2     -1     -1     -2     -2       3913       3102        689          0          0          0         0       689         0         0         0         0        1     young     bronze -0.333333\n",
       "1   2   120000.0  female          2   single   26     -1      2      0      0      0      2       2682       1725       2682       3272       3455       3261         0      1000      1000      1000         0      2000        1     young     silver  0.500000\n",
       "2   3    90000.0  female          2   single   34      0      0      0      0      0      0      29239      14027      13559      14331      14948      15549      1518      1500      1000      1000      1000      5000        0     young     silver  0.000000\n",
       "3   4    50000.0  female          2  married   37      0      0      0      0      0      0      46990      48233      49291      28314      28959      29547      2000      2019      1200      1100      1069      1000        0    middle     bronze  0.000000\n",
       "4   5    50000.0    male          2  married   57     -1      0     -1      0      0      0       8617       5670      35835      20940      19146      19131      2000     36681     10000      9000       689       679        0    senior     bronze -0.333333"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv file (last version)\n",
    "df = pd.read_csv(\"data/UCI_Credit_Card_for_feature_engineering.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc81e8",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# 2. Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ba098",
   "metadata": {},
   "source": [
    "The dataset contain categorical variables. Many machine learning algorithms can support categorical values without further manipulation but there are many more algorithms that do not. Therefore, we have to turn these text attributes into numerical values for further processing.\n",
    "\n",
    "An approach to encoding categorical values is to use a technique called **label encoding**. Label encoding is simply converting each value in a column to a number.\n",
    "\n",
    "One trick you can use in pandas is to convert a column to a category, then use those category values for your label encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb4375d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    female\n",
       "1    female\n",
       "2    female\n",
       "3    female\n",
       "4      male\n",
       "Name: sex, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sex'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c0fdec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sex'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9d3e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sex\"] = df[\"sex\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e910ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=['female', 'male'], ordered=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sex'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60001a54",
   "metadata": {},
   "source": [
    "Principal data and metedata are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40e78bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The levels are: Index(['female', 'male'], dtype='object')\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "29995    1\n",
      "29996    1\n",
      "29997    1\n",
      "29998    1\n",
      "29999    1\n",
      "Length: 30000, dtype: int8\n",
      "Is the variable ordered? False\n"
     ]
    }
   ],
   "source": [
    "print(f\"The levels are: {df['sex'].cat.categories}\")\n",
    "print(df['sex'].cat.codes)\n",
    "print(f\"Is the variable ordered? {df['sex'].cat.ordered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a739ae4",
   "metadata": {},
   "source": [
    "Then you can assign the encoded variable to a new column using the `cat.codes` accessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1f7e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>sex_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex  sex_cat\n",
       "0  female        0\n",
       "1  female        0\n",
       "2  female        0\n",
       "3  female        0\n",
       "4    male        1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sex_cat\"]=df[\"sex\"].cat.codes\n",
    "df[[\"sex\",\"sex_cat\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a21c6",
   "metadata": {},
   "source": [
    "Another trick is to use **LabelEncoder** function from **scikit-learn preprocessing** Python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "add05cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3132ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "Name: sex_cat, dtype: int32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sex_cat\"]=le.fit_transform(df['sex'])\n",
    "df[\"sex_cat\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c3e684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['female', 'female', 'female', ..., 'male', 'male', 'male'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can inverse label encoding\n",
    "le.inverse_transform(df[\"sex_cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4b20c",
   "metadata": {},
   "source": [
    "LabelEncoder can also be used to normalize numerical labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d1d04fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit_transform([1, 5, 5, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32932b",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Create a new dataframe and apply label encoding to \"credit_lev\" variable of previous dataframe.  \n",
    "Use both pandas and scikit-learn techniques.\n",
    "\n",
    "*(5 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c121c598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_lev</th>\n",
       "      <th>credit_lev_cat1</th>\n",
       "      <th>credit_lev_cat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bronze</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>silver</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>silver</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bronze</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bronze</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bronze</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gold</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>silver</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>silver</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>silver</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  credit_lev  credit_lev_cat1  credit_lev_cat2\n",
       "0     bronze                0                0\n",
       "1     silver                2                2\n",
       "2     silver                2                2\n",
       "3     bronze                0                0\n",
       "4     bronze                0                0\n",
       "5     bronze                0                0\n",
       "6       gold                1                1\n",
       "7     silver                2                2\n",
       "8     silver                2                2\n",
       "9     silver                2                2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write here your code\n",
    "df_1 = pd.read_csv(\"data/UCI_Credit_Card_for_feature_engineering.csv\")\n",
    "df_1[\"credit_lev_cat1\"] = df_1[\"credit_lev\"].astype(\"category\").cat.codes\n",
    "\n",
    "df_1[\"credit_lev_cat2\"] = le.fit_transform(df[\"credit_lev\"])\n",
    "df_1[[\"credit_lev\",\"credit_lev_cat1\",\"credit_lev_cat2\"]].head(10)\n",
    "#df_1[\"credit_lev\"].astype(\"category\").cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec67f6b",
   "metadata": {},
   "source": [
    "## Custom binary encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d322f",
   "metadata": {},
   "source": [
    "Depending on the data set, you may be able to create a binary column that meets your needs for further analysis.\n",
    "\n",
    "We can use `np.where()`, a numpy function to create a new column that indicates whether or not a specific condition is satisfied:\n",
    "\n",
    "*np.where(condition, x, y)*: return elements chosen from x or y depending on condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_over_50']=np.where(df[\"age\"]>50, 1, 0)\n",
    "df[['age','age_over_50']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af651e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_under_50']=np.where(df[\"age\"]<=50,1,0)\n",
    "df[['age','age_under_50']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd298c1c",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# 3. One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc06f0d8",
   "metadata": {},
   "source": [
    "Label encoding has the advantage that it is straightforward but it has the disadvantage that the numeric values can be “misinterpreted” by the algorithms. For example, the value of 0 is obviously less than the value of 1 but does it mean that category 0 is less important than category 1?\n",
    "\n",
    "A common alternative approach is called **one hot encoding**. The basic strategy is to convert each category value into a new column and assigns a 1 or 0 (True/False) value to the column. This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc5abe",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Create a new dataframe and apply one hot encoding to \"marriage\" variable. Create a binary column for each category.\n",
    "\n",
    "*(5 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a3f8f",
   "metadata": {},
   "source": [
    "Pandas supports one hot encoding using `get_dummies`. This function is named this way because it creates dummy/indicator variables (aka 1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aac206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select sex, marriage, credit level categorical variables\n",
    "df_cat=df[['sex','marriage','credit_lev']]\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply one hot encoding to all the variables\n",
    "df_dummies=pd.get_dummies(df_cat)\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b3b99",
   "metadata": {},
   "source": [
    "**Remember**: one-hot encoding of a categorical variable produces multiple variables that are highly correlated. You should leave out one column, in particular if you prepare data for linear models. For example, `sex_male` column is enough to explain sex variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply one hot encoding to specific columns\n",
    "pd.get_dummies(df_cat, columns=['sex','credit_lev']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can choose how to label the columns using prefix\n",
    "pd.get_dummies(df_cat, columns=['sex','credit_lev'], prefix=['sex','credit']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d7d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can ask for dropping one dummy column fo each categorical variable\n",
    "pd.get_dummies(df_cat, columns=['sex','credit_lev'], prefix=['sex','credit'], drop_first=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7fbe6",
   "metadata": {},
   "source": [
    "Scikit-learn also supports binary encoding by using the OneHotEncoder. The process of creating a pandas DataFrame adds a couple of extra steps. The key point is that you need to use toarray() to convert the results to a format that can be converted into a DataFrame. Therefore, pandas get_dummies() function is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b30b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "oe = preprocessing.OneHotEncoder()\n",
    "oe_results = oe.fit_transform(df[[\"credit_lev\"]])\n",
    "pd.DataFrame(oe_results.toarray(), columns=[\"credit_\" + i for i in oe.categories_]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eec360",
   "metadata": {},
   "source": [
    "One hot encoding is very useful but it can cause the number of columns to expand greatly if you have very many unique values in a column. For the number of values in this example, it is not a problem. However you can see how this gets really challenging to manage when you have many more options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074989",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "# 4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109abf7b",
   "metadata": {},
   "source": [
    "Feature scaling is performed on **numerical** variables to remove amplitude variation and improve machine learning models performance.\n",
    "\n",
    "This difference in scale for input variables does not affect all machine learning algorithms.\n",
    "\n",
    "Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Having features on a similar scale can help the gradient descent converge more quickly towards the minima.\n",
    "\n",
    "There are also algorithms that are unaffected by the scale of numerical input variables, most notably decision trees and ensembles of trees, like random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364928c",
   "metadata": {},
   "source": [
    "## Min-Max Scaling\n",
    "**Min-Max** is a scaling technique in which values are shifted and rescaled, so that they end up ranging **between 0 and 1**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9cd44",
   "metadata": {},
   "source": [
    "$$X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91164863",
   "metadata": {},
   "source": [
    "This can be achieved using `MinMaxScaler()` of scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489dab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df['age_norm']=min_max_scaler.fit_transform(df[['age']])\n",
    "df[['age','age_norm']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['age','age_norm']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9b71e",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "Standardization is a scaling technique where the values are centered around the mean with a unit standard deviation, so that the **mean** of observed values is **0** and the **standard deviation** is **1**. It is also known as **Z-Score**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad245e",
   "metadata": {},
   "source": [
    "$$z = \\frac{X — \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629d0f8",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Compute Z-score value of age variable using scipy.stats package. Then select the first element and check if the answer is correct\n",
    "\n",
    "*(5 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here \n",
    "first_value_z_score = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00765a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the answer with\n",
    "check_5_3(first_value_z_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d18c0d",
   "metadata": {},
   "source": [
    "The same result can be achieved using `StandardScaler()` of scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a32beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = preprocessing.StandardScaler()\n",
    "age_std=std_scaler.fit_transform(df[['age']])\n",
    "age_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_std']=age_std\n",
    "df[['age','age_std']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['age','age_std','age_norm']].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191d4cf",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Plot an appropriate visualization for standardized and normalized age distribution\n",
    "\n",
    "*(10 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e546fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd9ab42",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "# 5. Feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d81c5",
   "metadata": {},
   "source": [
    "<a id='5.1'></a>\n",
    "## Reduced Variance Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5645e82",
   "metadata": {},
   "source": [
    "VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\n",
    "\n",
    "As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by\n",
    "\n",
    "$$Var[X] = p(1-p) $$\n",
    "\n",
    "so we can select using the threshold `.8 * (1 - .8)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade5ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use one hot encoded variables as example\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_variance_model = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "X_selected_features_variance = feature_selection_variance_model.fit_transform(df_dummies)\n",
    "\n",
    "print(\"Original data set shape is \", df_dummies.shape)\n",
    "print(\"Reduced data set shape is \", X_selected_features_variance.shape)\n",
    "\n",
    "mask = feature_selection_variance_model.get_support() #list of booleans\n",
    "print(\"Selected features = \", df_dummies.columns[mask])\n",
    "print(\"Dropped features = \", df_dummies.columns[~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ca9fd",
   "metadata": {},
   "source": [
    "<a id='5.2'></a>\n",
    "## Univariate feature selection\n",
    "Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator.\n",
    "\n",
    "ANOVA F-test is an example of univariate statistical test.\n",
    "Analysis of variance (ANOVA) can determine whether the means of two or more groups are different. ANOVA uses F-tests to statistically test the equality of means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use credit card dataset\n",
    "numerical=['age','limit_bal','pay_avg','bill_amt1', 'bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5', 'bill_amt6', 'pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']\n",
    "df_num=df.loc[:,numerical]#numerical variables\n",
    "X=pd.concat([df_num,df_dummies],axis=1)\n",
    "y=df['default']#target variable\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af951c6",
   "metadata": {},
   "source": [
    "For instance, we can perform a ANOVA F-test to the samples to retrieve only the 4 best features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_univariate_model = SelectKBest(f_classif, k=4)#4 best features\n",
    "\n",
    "X_selected_features_univariate = feature_selection_univariate_model.fit_transform(X,y)\n",
    "\n",
    "print(\"Original data set shape is \", X.shape)\n",
    "print(\"Reduced data set shape is \",X_selected_features_univariate.shape)\n",
    "\n",
    "mask = feature_selection_univariate_model.get_support() #list of booleans\n",
    "print(\"Selected features = \",X.columns[mask])\n",
    "print(\"Dropped features = \", X.columns[~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b3a261",
   "metadata": {},
   "source": [
    "<a id='5.3'></a>\n",
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05136c39",
   "metadata": {},
   "source": [
    "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n",
    "\n",
    "So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d961b",
   "metadata": {},
   "source": [
    "PCA is mathematically a Singular Value Decomposition (SVD). We think of the SVD as decomposing matrix $X$ into orthogonal (i.e. independent) components with strengths given by the diagonal of $\\Sigma$,\n",
    "\n",
    "$$ \\Sigma = \\left[ \\begin{array}{ccc} \n",
    "\\sigma_1 & \\\\\n",
    "& \\sigma_2 & \\\\\n",
    "&& \\ddots & \\\\\n",
    "&&&\n",
    "\\end{array} \\right]$$\n",
    "\n",
    "By convention, we assume $|\\sigma_1| \\ge |\\sigma_2| \\ge \\cdots$.  One interpretation of this is that $\\sigma_1$ corresponds to the largest component of variation, $\\sigma_2$ corresponds to the second largest etc ... We often think of the smaller components as being just noise and the larger components as being being signal.  Therefore, it makes sense to truncate $\\Sigma$ to its largest $m$ components and just keep those.  In Scikit, this algorithm `sklearn.decomposition.PCA` returns $U \\Sigma P_m$ where $P_m$ is the projection operator onto the first $m$ dimensions.  (In reality, it performs a stochastic SVD, which should be faster than the exact SVD.)\n",
    "\n",
    "Geometrically, you can think about this as fitting an m-dimensional ellipsoid to the (originally p-dimensional) data. We expect that the majority of the variance in the data will be explained by the approximation, and any variation along the truncated axes is negligible. **It's important to note that we should choose $m$ according to how much of the variance we want to preserve vs. how much compression we want.**\n",
    "\n",
    "We can phrase an $m$ dimensional PCA as\n",
    "$$ \\min_{U, \\Sigma_m, V} \\| X - U \\Sigma_m V^T \\|_2 $$\n",
    "where $U$ is a unitary $n \\times n$ matrix, $\\Sigma_m$ is a diagonal $n \\times p$ matrix with rank $m$, and $V$ is a unitary $p \\times p$ matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99144285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1713e0c",
   "metadata": {},
   "source": [
    "We now apply principal component analysis. Since we need to decide how many component to select and for this purpose we apply PCA and plot the **explained variance ratio** (i.e. the percentage of variance explained by each of the selected components) and the cumulative explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732dcf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "n_features = 20\n",
    "n_info = 4\n",
    "\n",
    "X_pca, y_pca = make_classification(n_classes=2, class_sep=2, weights=[0.9, 0.1],\n",
    "                          n_informative=n_info, n_redundant=(n_features-n_info), flip_y=0.05,\n",
    "                          n_features=n_features, n_clusters_per_class=1,\n",
    "                          n_samples=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pca_model = PCA()\n",
    "#standardization\n",
    "X_std = preprocessing.StandardScaler().fit_transform(X_pca)\n",
    "full_fitted_model = full_pca_model.fit(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1215a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "axis_x = np.arange(1,len(full_fitted_model.explained_variance_ratio_)+1,1)\n",
    "plt.plot(axis_x, full_fitted_model.explained_variance_ratio_,'--o')\n",
    "plt.xticks(axis_x)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(axis_x, full_fitted_model.explained_variance_ratio_.cumsum(), '--o');\n",
    "plt.xticks(axis_x);\n",
    "plt.ylim(0,1.1)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c65365",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame([np.round(full_fitted_model.explained_variance_ratio_,2), \n",
    "               np.round(full_fitted_model.explained_variance_ratio_.cumsum(),2)],\n",
    "              index = ['Explained Variance Ratio', 'Cumulative Explained Variance Ratio'],\n",
    "              columns = ['PC_'+str(i) for i in range(1, len(full_fitted_model.explained_variance_ratio_)+1)]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d55070",
   "metadata": {},
   "source": [
    "As we can note, 4 components can actually explain most of the variance in the data so we apply PCA and select the first 4 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e6beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_df=pd.DataFrame(full_fitted_model.transform(X_std), \n",
    "                          columns = ['PC_'+str(i) for i in range(1, len(full_fitted_model.explained_variance_ratio_)+1)]\n",
    "                         ).iloc[:,:4]\n",
    "principal_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830cb8f0",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Plot an appropriate visualization of first two principal components\n",
    "\n",
    "*(5 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5015274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7578b7",
   "metadata": {},
   "source": [
    "<a id='5.4'></a>\n",
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9009990",
   "metadata": {},
   "source": [
    "Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting, and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.\n",
    "\n",
    "Feature selection using Random forest comes under the category of Embedded methods. Embedded methods combine the qualities of filter and wrapper methods. They are implemented by algorithms that have their own built-in feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c932b621",
   "metadata": {},
   "source": [
    "Random forests consist of multiple decision trees, each of them built over a random extraction of the observations from the dataset and a random extraction of the features. Not every tree sees all the features or all the observations, and this guarantees that the trees are de-correlated and therefore less prone to over-fitting. Each tree is also a sequence of yes-no questions based on a single or combination of features. At each node (this is at each question), the three divides the dataset into 2 buckets, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the importance of each feature is derived from how “pure” each of the buckets is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea39a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16036a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use credit card dataset\n",
    "numerical=['age','limit_bal','pay_avg','bill_amt1', 'bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5', 'bill_amt6', 'pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']\n",
    "df_num=df.loc[:,numerical]#numerical variables\n",
    "X=pd.concat([df_num,df_dummies],axis=1)\n",
    "y=df['default']#target variable\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=42)\n",
    "forest.fit(X, y)\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], X.columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28cf33b",
   "metadata": {},
   "source": [
    "A possible limitation of this technique is that correlated features will show in a tree similar and lowered importance, compared to what their importance would be if the tree was built without correlated counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc9bbc",
   "metadata": {},
   "source": [
    "<a id='5.5'></a>\n",
    "## Lasso regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17711665",
   "metadata": {},
   "source": [
    "Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.\n",
    "Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration.\n",
    "\n",
    "Here we will do feature selection using **Lasso**. Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is:\n",
    "\n",
    "$$(1 / (2 * nsamples)) * ||y - Xw||^2_2 + alpha * ||w||_1$$\n",
    "\n",
    "where $$||w||_1$$ is the regularization term\n",
    "\n",
    "If a feature is irrelevant, Lasso penalizes its regularization term and make it 0. Hence the features with term 0 are removed and the rest are taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d82531",
   "metadata": {},
   "source": [
    "In order to present this method, we use the **diabetes dataset** which is available from within scikit-learn (we can also print its description). Target variable is a quantitative measure (regression problem). Lasso regularization must be applied to regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2afbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc9e11",
   "metadata": {},
   "source": [
    "We are going to use the `Lasso` linear model from scikit-learn. We set a value of `alpha`, that is the constant that multiplies the regularization term. The model returns the parameter vector `coef_` (w in the cost function formula), aka Lasso coefficients. The features with the highest absolute `coef_` values are considered the most important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1169e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c78eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso=Lasso(alpha=0.1).fit(X,y)\n",
    "importance = np.abs(lasso.coef_)\n",
    "feature_names = np.array(diabetes.feature_names)\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.title(\"Feature importances via Lasso coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e8a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[importance==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1453d17",
   "metadata": {},
   "source": [
    "Variables \"age\", \"s2\", \"s4\" could be removed from the dataset, having coefficient zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ca7ca",
   "metadata": {},
   "source": [
    "<a id='5.6'></a>\n",
    "## Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeed4a0",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination (RFE) is a **wrapper-type** feature selection algorithm. This means that a different machine learning algorithm is wrapped by RFE and used to help select features.\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "We are going to use the `RFE` function from scikit-learn, which apply feature ranking with recursive feature elimination. We use the same **diabetes** scikit-learn dataset.The wrapped model is `LinearRegression`, which is an appropriate model to solve regression problems. RFE can also be applied to classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e36b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7719e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "selector = RFE(estimator=LinearRegression(), n_features_to_select=6)#The number of features to select\n",
    "selector = selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected features (equally important, with rank 1)\n",
    "feature_names[selector.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf2990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features that could be removed\n",
    "feature_names[selector.support_==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64342032",
   "metadata": {},
   "source": [
    "An important hyperparameter for the RFE algorithm is the number of features to select. In the previous section, we used an arbitrary number of selected features (6) which matches the number of informative features in the dataset. In practice, we cannot know the best number of features to select with RFE; instead, it is good practice to test different values.\n",
    "This can be achieved by performing cross-validation evaluation of different numbers of features and automatically selecting the number of features that resulted in the best mean score. The `RFECV` class implements this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c5151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb130ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv = RFECV(estimator=LinearRegression(), cv=StratifiedKFold(5),scoring='r2')\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (r2_score)\")\n",
    "plt.plot(range(1,len(rfecv.grid_scores_) + 1),rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected features (equally important, with rank 1)\n",
    "feature_names[rfecv.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features that could be removed\n",
    "feature_names[rfecv.support_==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97c997",
   "metadata": {},
   "source": [
    "Variables \"age\", \"s3\" and \"s6\" could be removed from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774cac0-1ea1-4dce-b069-e84872df7028",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "\n",
    "# 6. Unbalanced Classes\n",
    "\n",
    "In a binary classification problem, data is said to be unbalanced if there is a large difference in the number of samples representing each class. This situation is very common, arising each time we are dealing with an anomalous rare event. \n",
    "\n",
    "When two classes are balanced, a model predicting only one class for every input will be 50% accurate, i.e. will be correct every other time. As the imbalance increases, a model predicting the majority class will appear to be more and more accurate, which could have serious consequences if not properly diagnosed and handled. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c8470-9f29-4abb-afc4-a97fcf18d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/glemaitre/UnbalancedDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "# Generate some data\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.9, 0.1],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=120)\n",
    "\n",
    "# Instantiate a PCA object for the sake of easy visualisation\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# Fit and transform x to visualise inside a 2D feature space\n",
    "x_vis = pca.fit_transform(X)\n",
    "\n",
    "# Plot the two classes\n",
    "df_plot = pd.concat([pd.DataFrame(x_vis, columns = ['x_1', 'x_2']), pd.Series(y, name='target')], axis=1)\n",
    "sns.scatterplot(data=df_plot, x='x_1', y='x_2', hue='target', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d311c42-4f0f-4c93-8522-f37f8fca01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts(normalize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49685615-e87e-4a30-b970-7634beecf073",
   "metadata": {},
   "source": [
    "## Simple techniques to deal with unbalanced data\n",
    "\n",
    "There are several techniques to deal with unbalanced classes.\n",
    "\n",
    "## Collect more data\n",
    "Can you collect more samples of the minority class in order to have a better representation of it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49644fe5-170e-474c-b55a-9e2a6b6cdf30",
   "metadata": {},
   "source": [
    "## Undersampling\n",
    "When lots of data is present, a simple and effective strategy to deal with imbalance is to simply discard a random subset of the majority class, effectively undersampling it.\n",
    "- Pros: quick, easy to implement, lower load on model training because of less data\n",
    "- Cons: potentially losing important information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e435f-78ac-4ddc-98f4-57fbbd9c337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "US = RandomUnderSampler(sampling_strategy='majority', random_state=0)\n",
    "usx, usy = US.fit_resample(X, y)\n",
    "usx_vis = pca.transform(usx)\n",
    "\n",
    "# plot\n",
    "df_plot = pd.concat([pd.DataFrame(usx_vis, columns = ['x_1', 'x_2']), pd.Series(usy, name='target')], axis=1)\n",
    "sns.scatterplot(data=df_plot, x='x_1', y='x_2', hue='target', alpha=0.5)\n",
    "\n",
    "# plt.scatter(usx_vis[usy==0, 0], usx_vis[usy==0, 1], label=\"Class #0\", alpha=0.5,\n",
    "#             facecolor=palette[0], linewidth=0.15)\n",
    "# plt.scatter(usx_vis[usy==1, 0], usx_vis[usy==1, 1], label=\"Class #1\", alpha=0.5,\n",
    "#             facecolor=palette[2], linewidth=0.15)\n",
    "plt.title('Random under-sampling')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6ea92-57de-4768-8605-5535557946fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(usy).value_counts(normalize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972ea15-6dcd-4e28-a9ca-6015f613b03b",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "Similarly, using more copies of the data in the minority class can also be an effective strategy.\n",
    "- Pros: retaining all available information\n",
    "- Cons: more data, not necessarily more information if minority class is not well represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff78c7-020b-40c6-8dea-1530a7284fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "OS = RandomOverSampler(sampling_strategy='minority', random_state=0)\n",
    "osx, osy = OS.fit_resample(X, y)\n",
    "osx_vis = pca.transform(osx)\n",
    "\n",
    "# plot\n",
    "df_plot = pd.concat([pd.DataFrame(osx_vis, columns = ['x_1', 'x_2']), pd.Series(osy, name='target')], axis=1)\n",
    "sns.scatterplot(data=df_plot, x='x_1', y='x_2', hue='target', alpha=0.1)\n",
    "\n",
    "# plt.scatter(osx_vis[osy==0, 0], osx_vis[osy==0, 1], label=\"Class #0\", alpha=0.5,\n",
    "#             facecolor=palette[0], linewidth=0.15)\n",
    "# plt.scatter(osx_vis[osy==1, 0], osx_vis[osy==1, 1], label=\"Class #1\", alpha=0.5,\n",
    "#             facecolor=palette[2], linewidth=0.15)\n",
    "plt.title('Random over-sampling')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69e552-2d54-49a4-bf74-93da343a16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(osy).value_counts(normalize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775bbeee-8e9c-4a2f-89d5-bda30ec20394",
   "metadata": {},
   "source": [
    "## Synthetic data augmentation\n",
    "There are several techniques to create synthetic samples starting from the minority class. The most common are *SMOTE (Synthetic Minority Over-sampling Technique)*, and its variants. \n",
    "\n",
    "SMOTE creates synthetic samples from the minority class using a nearest neighbor technique. The algorithm selects two or more similar instances (using a distance measure) and it creates a new sample whose features are a linear combination of the neighbors features, with a random mixing parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305d49b-6c60-4139-b67d-fac94f0f8a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=0)\n",
    "# smote = SMOTE(sampling_strategy=1, random_state=0)\n",
    "smox, smoy = smote.fit_resample(X, y)\n",
    "smox_vis = pca.transform(smox)\n",
    "\n",
    "\n",
    "# plot\n",
    "df_plot = pd.concat([pd.DataFrame(smox_vis, columns = ['x_1', 'x_2']), pd.Series(smoy, name='target')], axis=1)\n",
    "sns.scatterplot(data=df_plot, x='x_1', y='x_2', hue='target', alpha=0.5)\n",
    "\n",
    "# plt.scatter(smox_vis[smoy==0, 0], smox_vis[smoy==0, 1], label=\"Class #0\", alpha=0.5,\n",
    "#             facecolor=palette[0], linewidth=0.15)\n",
    "# plt.scatter(smox_vis[smoy==1, 0], smox_vis[smoy==1, 1], label=\"Class #1\", alpha=0.5,\n",
    "#             facecolor=palette[2], linewidth=0.15)\n",
    "plt.title('SMOTE')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5ba47-6cbc-4a18-94df-3a89a5692f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(smoy).value_counts(normalize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f09abf",
   "metadata": {},
   "source": [
    "# Session completed\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb59c2",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d5bac",
   "metadata": {},
   "source": [
    "We have the following dataset **House Prices** from a well-known Kaggle competition (see details [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data))\n",
    "\n",
    "\n",
    "*(40 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f123db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house=pd.read_csv(\"data/house_prices.csv\")\n",
    "df_house.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc007cf",
   "metadata": {},
   "source": [
    "**1.** Select the **numeric** variables (*exclude target variable \"SalePrice\" and \"Id\" variable*)\n",
    "\n",
    "*(5 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba0b44",
   "metadata": {},
   "source": [
    "**2.** Handle missing values using **single inputation** method (**mean** substitution)\n",
    "\n",
    "*(5 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7ff56",
   "metadata": {},
   "source": [
    "**3.** Rank features using **Random Forest** classifier. Which is the most important feature?\n",
    "\n",
    "*(15 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b39d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eded59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3718a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert here the solution\n",
    "most_important_feature=\"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the answer with\n",
    "check_5_6_3(most_important_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d9237",
   "metadata": {},
   "source": [
    "**4.** Select the 8 most important features using **RFE** method (with Linear Regression estimator)\n",
    "\n",
    "*(10 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da83b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a28ed5",
   "metadata": {},
   "source": [
    "**5.** Plot the **correlation** matrix. Which is the feature with the highest correlation w.r.t. the target variable?\n",
    "\n",
    "*(5 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedda785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert here the solution\n",
    "most_correlated = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1271ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the answer with\n",
    "check_5_6_5(most_correlated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c7a846",
   "metadata": {},
   "source": [
    "### Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006b086",
   "metadata": {},
   "source": [
    "Now you will use a dataset very similar to the one just seen and you will perform a series of **features transformations**.\n",
    "\n",
    "*(40 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ames_feat_eng.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b5b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cea61e",
   "metadata": {},
   "source": [
    "**1.** Create the following features:\n",
    "\n",
    "- `LivLotRatio`: the ratio of `GrLivArea` to `LotArea`\n",
    "\n",
    "- `Spaciousness`: the sum of `FirstFlrSF` and `SecondFlrSF` divided by `TotRmsAbvGrd`\n",
    "\n",
    "- `TotalOutsideSF`: the sum of `WoodDeckSF`, `OpenPorchSF`, `EnclosedPorch`, `Threeseasonporch`, and `ScreenPorch`\n",
    "\n",
    "\n",
    "*(5 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00fd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code\n",
    "\n",
    "df[\"LivLotRatio\"] = ...\n",
    "df[\"Spaciousness\"] = ...\n",
    "df[\"TotalOutsideSF\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6272acb",
   "metadata": {},
   "source": [
    "**2.** We discovered an interaction between `BldgType` and `GrLivArea`. Create their interaction features.\n",
    "\n",
    "*(10 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b375bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061eabf1",
   "metadata": {},
   "source": [
    "**3.** Let's try creating a feature that describes how many kinds of outdoor areas a dwelling has. Create a feature `PorchTypes` that counts how many of the following are greater than `0.0`:\n",
    "\n",
    "```\n",
    "WoodDeckSF\n",
    "OpenPorchSF\n",
    "EnclosedPorch\n",
    "Threeseasonporch\n",
    "ScreenPorch\n",
    "```\n",
    "\n",
    "*(5 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9d69e",
   "metadata": {},
   "source": [
    "**4.** `MSSubClass` describes the type of a dwelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(df.MSSubClass.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d914700",
   "metadata": {},
   "source": [
    "You can see that there is a more general categorization described (roughly) by the first word of each category. \n",
    "\n",
    "Create a feature containing only these first words by splitting MSSubClass at the first underscore _.\n",
    "\n",
    "*(10 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af268672",
   "metadata": {},
   "source": [
    "**5.** The value of a home often depends on how it compares to typical homes in its neighborhood. \n",
    "\n",
    "Create a feature `MedNhbdArea` that describes the median of `GrLivArea` grouped on `Neighborhood`.\n",
    "\n",
    "*(10 min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
